{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and visualize\n",
    "Process\n",
    " 1. Read data from wikipedia\n",
    " 2. Create nx.graph\n",
    " 3. Visuzlize\n",
    "\n",
    "three options for reading data,\n",
    "- option 0:directory from python dict\n",
    "- option 1:from rdb\n",
    "- option 2:from elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wikiscraper import WikiScraper\n",
    "data = pd.read_csv(\"contents_test.csv\",index_col=0)\n",
    "wscs = []\n",
    "\n",
    "j = 0\n",
    "start = 0\n",
    "stop = start+1+len(data.index.values)\n",
    "\n",
    "for i in data.index.values:\n",
    "    j+=1\n",
    "    if j<start:\n",
    "        continue\n",
    "        \n",
    "    wsc = WikiScraper()\n",
    "    wsc.load_wiki(i.decode('utf-8'), pageid=data.at[i, 'id'], lang='ja')\n",
    "#         wsc.load_wiki(i.decode('utf-8'), 'ja')\n",
    "# #     wsc.load_html(p.html())\n",
    "    if wsc._bsObj is not None:\n",
    "        wsc.get_list_from_headline(u'.*スタッフ')\n",
    "        wsc.get_table('infobox')\n",
    "    \n",
    "    wscs.append(wsc)\n",
    "    \n",
    "    print '-------------------------------------'\n",
    "    print wsc\n",
    "    print '+++++++++++++++++++++'\n",
    "#     print wsc._name\n",
    "\n",
    "    if j>=stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Create nx.graph\n",
    "\n",
    "three options for reading data,\n",
    "- option 0:directory from python dict\n",
    "- option 1:from rdb\n",
    "- option 2:from elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 0 directory create nx.graph from python dict\n",
    "\n",
    "threshold = 1.0\n",
    "max_number_of_nodes = 500\n",
    "count_once = True\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "key_rm = [u'放送期間', u'放送時間', u'公開', u'上映時間', u'次作', u'回数', u'放送分']\n",
    "value_rm = [u'', u'同上', u'日本', u'日本語', u'英語', u'公式サイト', u'ほか', u'ステレオ放送', u'文字多重放送',u'歴代エンディングテーマを参照',\n",
    "            u'フジテレビ番組基本情報']\n",
    "\n",
    "for wsc in wscs:\n",
    "    \n",
    "    point = float(data.at[wsc._name.encode('utf-8'), 'ポイント'])\n",
    "    G.add_node(wsc._name, genre='content', point=point)\n",
    "    attrs = set()\n",
    "    for k, v in wsc._result.items():\n",
    "        #add first genre to node attribute\n",
    "        if k == u'ジャンル':\n",
    "            if len(v)==0:\n",
    "                G.node[wsc._name]['genre'] = v\n",
    "            else:\n",
    "                G.node[wsc._name]['genre'] = v[0]\n",
    "        elif k not in key_rm:\n",
    "            for attr in v:\n",
    "                if attr not in data.index.values and attr not in value_rm: #do nothing for contents itself\n",
    "                    if G.node.get(attr) is None: #first time\n",
    "                        G.add_node(attr, genre='attribute', point=point)\n",
    "                    elif not (attr in attrs and count_once): # from second time\n",
    "                        G.node[attr]['point'] += point\n",
    "                    G.add_edge(wsc._name, attr, relation=k) \n",
    "                    attrs.add(attr)\n",
    "    print wsc._name, len(G.nodes)\n",
    "        \n",
    "cliques = nx.find_cliques(G)\n",
    "# #標準出力\n",
    "# for c in cliques:\n",
    "#     print c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1 use database\n",
    "# 1-1 put data into db\n",
    "\n",
    "#Write python dict to database\n",
    "#use psycopg not pandas nor sqlalchemy since, column is dynamic\n",
    "#temporary ignore captial or not\n",
    "import traceback\n",
    "from dbcontrol import DBControl\n",
    "\n",
    "table_name = 'contents'\n",
    "dbc = DBControl(dbname='testdb', host='localhost', user='pgadmin', password='password')\n",
    "dbc.create_table(table_name)\n",
    "i = 0\n",
    "# print get_table_name()\n",
    "for wsc in wscs:\n",
    "    print i,\n",
    "    i+=1\n",
    "    name = value_pre_process(wsc._name) \n",
    "    dbc.insert(table_name, 'name', name)\n",
    "    for k, v in wsc._result.items():\n",
    "#         print '--------------------------------------'\n",
    "#         print k, ','.join(set(v))\n",
    "        k = key_pre_process(k)\n",
    "        value = value_pre_process(','.join(set(v)))\n",
    "        if (k,) not in dbc.get_columns_name(table_name):\n",
    "            try:\n",
    "                dbc.alter(table_name, k, 'text')\n",
    "            except:\n",
    "                print 'error in alter key:', name,  k\n",
    "                traceback.print_exc()\n",
    "                dbc._cur.execute('ABORT;')\n",
    "                continue\n",
    "        try:\n",
    "            dbc.update(table_name, k, value, name)\n",
    "        except:\n",
    "            print 'error in update value:', name, k, value \n",
    "            traceback.print_exc()\n",
    "            dbc._cur.execute('ABORT;')\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1 use database\n",
    "# 1-2 create graph from db\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from dbcontrol import DBControl, key_pre_process, value_pre_process\n",
    "\n",
    "G = nx.Graph()\n",
    "count_once=True\n",
    "key_rm = [u'放送期間', u'放送時間', u'公開', u'上映時間', u'次作', u'回数', u'放送分']\n",
    "value_rm = [u'', u'同上', u'日本', u'日本語', u'英語', u'公式サイト', u'ほか', u'ステレオ放送', u'文字多重放送',u'歴代エンディングテーマを参照',\n",
    "            u'フジテレビ番組基本情報']\n",
    "\n",
    "#1. read name and point from csv\n",
    "input_data = pd.read_csv(\"contents_test.csv\",index_col=0)\n",
    "names = input_data.index.values\n",
    "\n",
    "#2. get data from database\n",
    "table_name = 'contents'\n",
    "dbc = DBControl(dbname='testdb', host='localhost', user='pgadmin', password='password')\n",
    "\n",
    "#3. separate key and value\n",
    "#4. calc\n",
    "columns = dbc.get_columns_name(table_name)\n",
    "# name = 'ウンナンさん'\n",
    "for name in names:\n",
    "    point = float(input_data.at[name, 'ポイント'])\n",
    "    G.add_node(name, genre='content', point=point)\n",
    "    attrs = set()\n",
    "\n",
    "    print '--------',name, point, '----------'\n",
    "    for column in columns:\n",
    "        if column[0]=='name':\n",
    "            continue\n",
    "        res = dbc.get_value(table_name, key='name', value=value_pre_process(name.decode('utf-8')), column=column[0])\n",
    "        k = column[0].decode('utf-8')\n",
    "        if len(res)>0:\n",
    "            val = res[0][0]\n",
    "            if val is not None:\n",
    "#                 print 'key:', k,[k]\n",
    "                val = val.decode('utf-8')\n",
    "                v = val.split(u'，')\n",
    "                if k == u'ジャンル':\n",
    "                    if len(v)==0:\n",
    "                        G.node[name]['genre'] = v\n",
    "                    else:\n",
    "                        G.node[name]['genre'] = v[0]\n",
    "                elif k not in key_rm:\n",
    "                    for attr in v:\n",
    "#                         print '   value:', attr, [attr]\n",
    "                        if attr not in names and attr not in value_rm: #do nothing for contents itself\n",
    "                            if G.node.get(attr) is None: #first time\n",
    "                                G.add_node(attr, genre='attribute', point=point)\n",
    "                            elif not (attr in attrs and count_once): # from second time\n",
    "                                G.node[attr]['point'] += point\n",
    "                            G.add_edge(name, attr, relation=k) \n",
    "                            attrs.add(attr)\n",
    "\n",
    "#                 print col, '::', \n",
    "#                 for v in val.split('，'):\n",
    "#                     print v, ' = ',\n",
    "#                 print\n",
    "        else:\n",
    "            pass #todo not in the database\n",
    "\n",
    "        \n",
    "#to confirm\n",
    "# for e in G.edgfrom dbcontrol import DBControl\n",
    "# else:\n",
    "#     print e[0], e[1]\n",
    "# for n in G.nodes:\n",
    "#     print n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#option2 use elasticsearch\n",
    "# 2-1 put data into elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "table_name = 'contents'\n",
    "# mapping = {\n",
    "#   \"mappings\": {\n",
    "#     \"contents\": {\n",
    "#       \"properties\": {\n",
    "#         \"name\": {\n",
    "#           \"type\":     \"keyword\",\n",
    "# #           \"analyzer\": \"kuromoji\",\n",
    "#           \"index\" : \"not_analyzed\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "i = 0\n",
    "# print get_table_name()\n",
    "for wsc in wscs:\n",
    "    print i,\n",
    "    i+=1\n",
    "    res_dict = wsc._result\n",
    "    print [wsc._name]\n",
    "    res_dict['name'] = wsc._name \n",
    "    es.index(index=\"my-index\", body=res_dict, doc_type=\"_doc\")\n",
    "#     print res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from elasticsearch\n",
    "# 2-2 create nx.graph from elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "res = es.search(index=\"my-index\", body=search_query)\n",
    "for hit in res['hits']['hits']:\n",
    "    print '-------------'\n",
    "    print hit['_source']['name']\n",
    "    for k, v in hit['_source'].items():\n",
    "        if k != 'name':\n",
    "            print k,\n",
    "            for value in v:\n",
    "                print value, ', ',\n",
    "            print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "ウンナンさん\n",
      "-------------\n",
      "ウンナンの気分は上々。\n",
      "-------------\n",
      "UN街\n",
      "-------------\n",
      "勇者ヨシヒコと魔王の城\n",
      "-------------\n",
      "勇者ヨシヒコと悪霊の鍵\n",
      "-------------\n",
      "勇者ヨシヒコと導かれし七人\n",
      "-------------\n",
      "ネリさまぁ〜ず\n",
      "-------------\n",
      "神さまぁ〜ず\n",
      "-------------\n",
      "ホリさまぁ〜ず\n",
      "-------------\n",
      "マルさまぁ〜ず\n",
      "-------------\n",
      "バナナ塾\n",
      "-------------\n",
      "バナナマンのブログ刑事\n",
      "-------------\n",
      "オトナ養成所_バナナスクール\n",
      "-------------\n",
      "ツギクルもん\n",
      "-------------\n",
      "うつけもん\n",
      "-------------\n",
      "オサレもん\n",
      "-------------\n",
      "30minutes\n",
      "-------------\n",
      "30minutes鬼\n",
      "-------------\n",
      "30minutes鬼\n",
      "-------------\n",
      "デリパンダ〜おしゃべりデリ坊、東京ド真ん中配達中〜\n",
      "-------------\n",
      "あらびき団\n",
      "-------------\n",
      "タイプライターズ\n",
      "-------------\n",
      "飛び出せ!科学くん\n",
      "-------------\n",
      "ラブレターズのオールナイトニッポン0(ZERO)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinh/.local/lib/python2.7/site-packages/ipykernel_launcher.py:58: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "#read from elasticsearch\n",
    "# 2-2 create nx.graph from elasticsearch\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "count_once=True\n",
    "key_rm = [u'放送期間', u'放送時間', u'公開', u'上映時間', u'次作', u'回数', u'放送分']\n",
    "value_rm = [u'', u'同上', u'日本', u'日本語', u'英語', u'公式サイト', u'ほか', u'ステレオ放送', u'文字多重放送',u'歴代エンディングテーマを参照',\n",
    "            u'フジテレビ番組基本情報']\n",
    "\n",
    "#1. read name and point from csv\n",
    "input_data = pd.read_csv(\"contents_test.csv\",index_col=0)\n",
    "names = input_data.index.values\n",
    "\n",
    "#2. get data from elastic search\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "# res = es.search(index=\"my-index\", body={\"query\": {\"match_all\": {}}})\n",
    "# res = es.search(index=\"my-index\", body={\"query\": {\"match\": {\"name\":u'ウンナンさん'}}})\n",
    "# res = es.search(index=\"my-index\", body={\"query\": {\"term\": {'name': u\"ウンナンさん\"} }})\n",
    "search_query = {\n",
    "    \"query\":{\n",
    "        \"query_string\":{\n",
    "            \"default_field\" : \"name\",\n",
    "            \"query\":\"\\\"ウンナンさん\\\"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    point = float(input_data.at[name, 'ポイント'])\n",
    "    G.add_node(name, genre='content', point=point)\n",
    "    attrs = set()\n",
    "    search_query['query']['query_string']['query'] = '\\\"' + name + '\\\"'\n",
    "    res = es.search(index=\"my-index\", body=search_query)\n",
    "    for hit in res['hits']['hits']:\n",
    "        print '-------------'\n",
    "        print hit['_source']['name']\n",
    "        for k, v in hit['_source'].items():\n",
    "#             if k != 'name':\n",
    "#                 print k,\n",
    "#                 for value in v:\n",
    "#                     print value, ', ',\n",
    "#                 print\n",
    "            #add first genre to node attribute\n",
    "            if k == u'ジャンル':\n",
    "                if len(v)==0:\n",
    "                    G.node[name]['genre'] = v\n",
    "                else:\n",
    "                    G.node[name]['genre'] = v[0]\n",
    "            elif k not in key_rm and  k != 'name':\n",
    "                for attr in v:\n",
    "                    if attr not in names and attr not in value_rm: #do nothing for contents itself\n",
    "                        if G.node.get(attr) is None: #first time\n",
    "                            G.add_node(attr, genre='attribute', point=point)\n",
    "                        elif not (attr in attrs and count_once): # from second time\n",
    "                            G.node[attr]['point'] += point\n",
    "                        \n",
    "                        G.add_edge(name, attr, relation=k) \n",
    "                        attrs.add(attr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visuzlize\n",
    "\n",
    "1. remove nodes \n",
    "2. visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  1.0\n",
      " node num before simplify: 645\n",
      " node num after simplify: 42\n"
     ]
    }
   ],
   "source": [
    "# remove nodes for visualization\n",
    "threshold = 1.0\n",
    "max_number_of_nodes = 500\n",
    "count_once = True\n",
    "first = True\n",
    "while len(G.nodes)>max_number_of_nodes or first:\n",
    "    print 'Threshold: ', threshold\n",
    "    print ' node num before simplify:', len(G.nodes)\n",
    "    value_rm.extend([node for node,degree in dict(G.degree()).items() if degree < 2 and G.node[node]['genre'] is 'attribute']) #remove node based on degree\n",
    "    value_rm.extend([node for node in G.nodes if  G.node[node]['point'] < threshold and G.node[node]['genre'] is 'attribute'])\n",
    "    G.remove_nodes_from(value_rm)\n",
    "    print ' node num after simplify:', len(G.nodes)\n",
    "    threshold += 0.1\n",
    "    first = False\n",
    "\n",
    "#mix the closed genres\n",
    "for v in data.index.values:\n",
    "    title = v.decode('utf-8')\n",
    "    if title in G.node:\n",
    "        if re.search( u'(.*バラエティ.*)|(.*お笑い.*)', G.node[title]['genre']):\n",
    "            G.node[title]['genre'] = u'バラエティ'\n",
    "        if re.search( u'.*ドラマ.*', G.node[title]['genre']):\n",
    "            G.node[title]['genre'] = u'ドラマ'\n",
    "        if re.search( u'.*SF.*', G.node[title]['genre']):\n",
    "            G.node[title]['genre'] = u'SF'\n",
    "\n",
    "# nx.draw_networkx(G,font_family='AppleGothic',font_size =8)\n",
    "# plt.show()\n",
    "# nx.write_gexf(G, 'result.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinh/.local/lib/python2.7/site-packages/ipykernel_launcher.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#visualize with pyvis + networkx \n",
    "from pyvis.network import Network\n",
    "# nxg = nx.complete_graph(10)\n",
    "# gg = Network(notebook=True)\n",
    "# gg.from_nx(G)\n",
    "# gg.show(\"nx.html\")\n",
    "\n",
    "def get_net_node(net, key, value):\n",
    "    for item in net.nodes:\n",
    "        if item[key] == value:\n",
    "            return item\n",
    "    return None \n",
    "\n",
    "net = Network(height='500px', width='1000px')\n",
    "# net = Network(notebook=True)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.force_atlas_2based(\n",
    "    gravity=-30, \n",
    "    central_gravity=0.01, \n",
    "    spring_length=1.0, \n",
    "    spring_strength=0.8, \n",
    "    damping=1.0, \n",
    "    overlap=1.0)\n",
    "# net.options.physics.__dict__['timestep'] = 0.5\n",
    "net.options.physics.__dict__['adaptiveTimestep'] = True\n",
    "net.toggle_hide_edges_on_drag(True)\n",
    "\n",
    "scaling = dict(min=1, \n",
    "                       max=100, \n",
    "                       label=dict(enable=True, \n",
    "                                          min=10, \n",
    "                                          max=100))\n",
    "\n",
    "node_id = 0\n",
    "for label in G.nodes:\n",
    "#     print label, G.node[label]['genre'], G.node[label]['point']\n",
    "    if G.node[label]['genre'] == 'attribute':\n",
    "        shape = 'dot'\n",
    "        value = G.node[label]['point']\n",
    "        mass=G.node[label]['point']\n",
    "    else:\n",
    "        shape = 'box'\n",
    "        value = threshold\n",
    "        mass = 10\n",
    "    net.add_node(node_id, label=label, \n",
    "                 group=G.node[label]['genre'], \n",
    "                 value=value,\n",
    "                 mass=mass,\n",
    "                 scaling = scaling,\n",
    "                 shape=shape)\n",
    "    node_id += 1\n",
    "    \n",
    "for edge in G.edges:\n",
    "    edge[0], edge[1]\n",
    "    net.add_edge( get_net_node(net, 'label', edge[0])['id'], \n",
    "                 get_net_node(net, 'label', edge[1])['id'])\n",
    "    \n",
    "# net.save_graph(\"result.html\")\n",
    "net.show(\"result.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
